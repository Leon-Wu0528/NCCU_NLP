{"cells":[{"cell_type":"code","source":["import json\n","import numpy as np\n","import tensorflow as tf\n","from transformers import BertTokenizer, TFBertForSequenceClassification\n","from tqdm import tqdm\n","\n","def is_match(claim, text):\n","    # 使用 BERT tokenizer 对句子进行编码\n","    claim_tokens = tokenizer.tokenize(claim)\n","    text_tokens = tokenizer.tokenize(text)\n","\n","    # 将句子转换为输入的特征\n","    inputs = tokenizer.encode_plus(\n","        claim_tokens,\n","        text_tokens,\n","        add_special_tokens=True,\n","        max_length=512,\n","        padding='max_length',\n","        truncation=True,\n","        return_tensors='tf'\n","    )\n","\n","    # 获取输入特征的嵌入表示\n","    input_ids = inputs['input_ids']\n","    attention_mask = inputs['attention_mask']\n","    embeddings = loaded_model.bert(input_ids, attention_mask=attention_mask)[0]\n","\n","    # 计算句子的平均嵌入表示\n","    claim_embedding = tf.reduce_mean(embeddings[0], axis=0)\n","    text_embedding = tf.reduce_mean(embeddings[1:], axis=0)\n","\n","    # 计算句子的余弦相似度\n","    similarity = tf.keras.losses.cosine_similarity(claim_embedding, text_embedding)\n","\n","    # 根据相似度阈值判断是否匹配\n","    similarity_threshold = 0.7\n","    return similarity >= similarity_threshold\n","\n","print(\"載入模型\")\n","loaded_model = TFBertForSequenceClassification.from_pretrained('bert_model')\n","\n","print(\"測試資料預處理\")\n","test_data = []\n","with open('public_test.jsonl', 'r', encoding='utf-8') as f:\n","    for line in tqdm(f):\n","        entry = json.loads(line)\n","        test_data.append({'id': entry['id'], 'claim': entry['claim']})\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n","\n","test_input_ids = []\n","test_attention_masks = []\n","\n","for entry in tqdm(test_data):\n","    claim = entry['claim']\n","    encoded = tokenizer.encode_plus(\n","        claim,\n","        add_special_tokens=True,\n","        max_length=512,\n","        padding='max_length',\n","        truncation=True,\n","        return_attention_mask=True,\n","        return_tensors='tf'\n","    )\n","    test_input_ids.append(encoded['input_ids'])\n","    test_attention_masks.append(encoded['attention_mask'])\n","\n","test_input_ids = tf.concat(test_input_ids, axis=0)\n","test_attention_masks = tf.concat(test_attention_masks, axis=0)\n","\n","print(\"進行預測\")\n","predictions = loaded_model.predict(\n","    x={'input_ids': test_input_ids, 'attention_mask': test_attention_masks}\n",")\n","\n","print(\"根據預測結果進行後續處理\")\n","output_data = []\n","for i, entry in enumerate(test_data):\n","    prediction = predictions.logits[i]\n","    predicted_label = ''\n","    predicted_evidence = []\n","\n","    if int(prediction.argmax()) == 2:  # 如果預測為 \"not enough info\"\n","        predicted_label = 'not enough info'\n","    else:\n","        for j in range(1, 25):  # 搜尋 wiki-001.jsonl 到 wiki-024.jsonl\n","            wiki_file = f'wiki-{str(j).zfill(3)}.jsonl'\n","            with open(wiki_file, 'r', encoding='utf-8') as wiki_f:\n","                for line_num, line in enumerate(wiki_f):\n","                    wiki_entry = json.loads(line)\n","                    text = wiki_entry['text']\n","                    if is_match(entry['claim'], text):\n","                        predicted_label = 'supports' if int(prediction.argmax()) == 0 else 'refutes'\n","                        predicted_evidence.append([wiki_entry['id'], line_num])\n","                        break\n","            if len(predicted_evidence) >= 5:\n","                break\n","\n","    output_data.append({\n","        'id': entry['id'],\n","        'predicted_label': predicted_label,\n","        'predicted_evidence': predicted_evidence\n","    })\n","\n","output_file = 'predictions.jsonl'\n","with open(output_file, 'w', encoding='utf-8') as f:\n","    for entry in output_data:\n","        f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n","\n","print('預測結果輸出完成')"],"metadata":{"id":"tCaE7conIP1S"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"歡迎使用 Colaboratory","toc_visible":true,"provenance":[{"file_id":"/v2/external/notebooks/intro.ipynb","timestamp":1685340661152}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}